{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49805cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.utils import resample\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, BatchNormalization, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML\n",
    "import pickle\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load the CICIDS 2018 dataset\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from {file_path}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Data loaded successfully in {time.time() - start_time:.2f} seconds!\")\n",
    "        print(f\"Dataset shape: {df.shape}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def perform_eda(df):\n",
    "    \"\"\"\n",
    "    Perform exploratory data analysis on the dataset\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EXPLORATORY DATA ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    \n",
    "    print(\"\\nBasic Information:\")\n",
    "    print(f\"Number of samples: {df.shape[0]}\")\n",
    "    print(f\"Number of features: {df.shape[1]}\")\n",
    "    \n",
    "    \n",
    "    print(\"\\nMissing values summary:\")\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(f\"Total features with missing values: {sum(missing_values > 0)}\")\n",
    "    if sum(missing_values > 0) > 0:\n",
    "        print(missing_values[missing_values > 0])\n",
    "    \n",
    "    \n",
    "    print(\"\\nAttack distribution:\")\n",
    "    attack_counts = df['Label'].value_counts()\n",
    "    print(attack_counts)\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.countplot(x='Label', data=df)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    ax.set(xlabel='Attack Type', ylabel='Number of Attacks')\n",
    "    plt.title('Distribution of Network Intrusion Types')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    print(\"\\nData types:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    \n",
    "    \n",
    "    print(\"\\nGenerating correlation heatmap for key features...\")\n",
    "    numeric_df = df.select_dtypes(include=[np.number]).iloc[:, :10]  \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "    plt.title('Correlation Heatmap of Key Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    print(\"\\nGenerating interactive scatter plot...\")\n",
    "    sample_df = df.sample(min(10000, len(df)))\n",
    "    fig = px.scatter(sample_df, \n",
    "                     x='Flow Duration', \n",
    "                     y='Tot Fwd Pkts',  \n",
    "                     color='Label',\n",
    "                     title='Flow Duration vs Total Forward Packets',\n",
    "                     opacity=0.7)\n",
    "    fig.show()\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_data(df, balanced_sample_size=20000):\n",
    "    \"\"\"\n",
    "    Preprocess the data for model training\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"DATA PREPROCESSING\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    \n",
    "    print(\"\\nHandling missing values...\")\n",
    "    df_cleaned = df.dropna()\n",
    "    print(f\"Shape after dropping missing values: {df_cleaned.shape}\")\n",
    "    \n",
    "    \n",
    "    print(\"\\nEncoding labels...\")\n",
    "    label_encoder = LabelEncoder()\n",
    "    df_cleaned['Label'] = label_encoder.fit_transform(df_cleaned['Label'])\n",
    "    print(f\"Unique encoded labels: {df_cleaned['Label'].unique()}\")\n",
    "    print(f\"Label mapping: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")\n",
    "    \n",
    "    \n",
    "    joblib.dump(label_encoder, 'label_encoder.pkl')\n",
    "    print(\"Label encoder saved to 'label_encoder.pkl'\")\n",
    "    \n",
    "    \n",
    "    print(\"\\nBalancing the dataset...\")\n",
    "    \n",
    "    label_frames = []\n",
    "    unique_labels = df_cleaned['Label'].unique()\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        label_df = df_cleaned[df_cleaned['Label'] == label]\n",
    "        if len(label_df) > balanced_sample_size:\n",
    "            label_df = resample(label_df, n_samples=balanced_sample_size, random_state=42, replace=False)\n",
    "        else:\n",
    "            label_df = resample(label_df, n_samples=balanced_sample_size, random_state=42, replace=True)\n",
    "        label_frames.append(label_df)\n",
    "    \n",
    "    \n",
    "    balanced_df = pd.concat(label_frames)\n",
    "    print(f\"Shape of balanced dataset: {balanced_df.shape}\")\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(x='Label', data=balanced_df)\n",
    "    plt.title('Distribution After Balancing')\n",
    "    plt.xlabel('Attack Type (Encoded)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    print(\"\\nRemoving non-feature columns...\")\n",
    "    columns_to_drop = ['Timestamp', 'Label']\n",
    "    \n",
    "    for col in balanced_df.columns:\n",
    "        if balanced_df[col].dtype == 'object' and col != 'Label':\n",
    "            columns_to_drop.append(col)\n",
    "    \n",
    "    \n",
    "    y = balanced_df['Label']\n",
    "    X = balanced_df.drop(columns=columns_to_drop, errors='ignore')\n",
    "    \n",
    "    print(f\"Features shape after dropping non-feature columns: {X.shape}\")\n",
    "    \n",
    "    \n",
    "    print(\"\\nChecking for infinity or extremely large values...\")\n",
    "    \n",
    "    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    \n",
    "    nan_count = X.isna().sum().sum()\n",
    "    if nan_count > 0:\n",
    "        print(f\"Found {nan_count} NaN values after replacing infinities.\")\n",
    "        \n",
    "        \n",
    "        for col in X.columns:\n",
    "            if X[col].isna().sum() > 0:\n",
    "                median_val = X[col].median()\n",
    "                X[col].fillna(median_val, inplace=True)\n",
    "    \n",
    "    \n",
    "    print(\"\\nClipping extremely large values...\")\n",
    "    for col in X.columns:\n",
    "        q1 = X[col].quantile(0.01)\n",
    "        q3 = X[col].quantile(0.99)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 3 * iqr\n",
    "        upper_bound = q3 + 3 * iqr\n",
    "        \n",
    "        X[col] = X[col].clip(lower_bound, upper_bound)\n",
    "    \n",
    "    \n",
    "    print(\"\\nNormalizing features...\")\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    \n",
    "    joblib.dump(scaler, 'feature_scaler.pkl')\n",
    "    print(\"Feature scaler saved to 'feature_scaler.pkl'\")\n",
    "    \n",
    "    \n",
    "    y_onehot = to_categorical(y, num_classes=len(unique_labels))\n",
    "    \n",
    "    \n",
    "    print(\"\\nSplitting data into training and testing sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    X_train_onehot, X_test_onehot, y_train_onehot, y_test_onehot = train_test_split(\n",
    "        X_scaled, y_onehot, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    \n",
    "    X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "    X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "    \n",
    "    print(f\"Training set shape: {X_train.shape}\")\n",
    "    print(f\"Testing set shape: {X_test.shape}\")\n",
    "    print(f\"CNN training set shape: {X_train_cnn.shape}\")\n",
    "    \n",
    "    \n",
    "    feature_names = X.columns.tolist()\n",
    "    with open('feature_names.pkl', 'wb') as f:\n",
    "        pickle.dump(feature_names, f)\n",
    "    print(\"Feature names saved to 'feature_names.pkl'\")\n",
    "    \n",
    "    return {\n",
    "        'X_train': X_train, \n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train, \n",
    "        'y_test': y_test,\n",
    "        'X_train_cnn': X_train_cnn,\n",
    "        'X_test_cnn': X_test_cnn,\n",
    "        'y_train_onehot': y_train_onehot,\n",
    "        'y_test_onehot': y_test_onehot,\n",
    "        'unique_labels': unique_labels,\n",
    "        'feature_names': feature_names,\n",
    "        'label_encoder': label_encoder\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def build_rf_model(data_dict):\n",
    "    \"\"\"\n",
    "    Build and train a Random Forest model\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"RANDOM FOREST MODEL TRAINING\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    X_train = data_dict['X_train']\n",
    "    y_train = data_dict['y_train']\n",
    "    X_test = data_dict['X_test']\n",
    "    y_test = data_dict['y_test']\n",
    "    feature_names = data_dict['feature_names']\n",
    "    \n",
    "    print(\"\\nTraining Random Forest model...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=20,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Model trained in {training_time:.2f} seconds\")\n",
    "    \n",
    "    \n",
    "    print(\"\\nEvaluating model on test data...\")\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix - Random Forest')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    if len(feature_names) > 0:\n",
    "        importances = rf_model.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.title('Feature Importances - Random Forest')\n",
    "        plt.bar(range(min(20, len(feature_names))), \n",
    "                importances[indices][:20],\n",
    "                align='center')\n",
    "        plt.xticks(range(min(20, len(feature_names))), \n",
    "                  [feature_names[i] for i in indices[:20]], \n",
    "                  rotation=90)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    joblib.dump(rf_model, 'random_forest_model.pkl')\n",
    "    print(\"Random Forest model saved to 'random_forest_model.pkl'\")\n",
    "    \n",
    "    return rf_model\n",
    "\n",
    "\n",
    "def build_cnn_model(data_dict):\n",
    "    \"\"\"\n",
    "    Build and train a CNN model\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"CNN MODEL TRAINING\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    X_train_cnn = data_dict['X_train_cnn']\n",
    "    y_train_onehot = data_dict['y_train_onehot']\n",
    "    X_test_cnn = data_dict['X_test_cnn']\n",
    "    y_test_onehot = data_dict['y_test_onehot']\n",
    "    \n",
    "    \n",
    "    n_timesteps = X_train_cnn.shape[1]\n",
    "    n_features = 1\n",
    "    n_outputs = y_train_onehot.shape[1]\n",
    "    \n",
    "    print(f\"Input shape: ({n_timesteps}, {n_features})\")\n",
    "    print(f\"Output shape: {n_outputs}\")\n",
    "    \n",
    "    \n",
    "    print(\"\\nBuilding CNN model...\")\n",
    "    model = Sequential()\n",
    "    \n",
    "    \n",
    "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n",
    "                   padding='same', input_shape=(n_timesteps, n_features)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=3, strides=2, padding='same'))\n",
    "    \n",
    "    \n",
    "    model.add(Conv1D(filters=128, kernel_size=6, activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=3, strides=2, padding='same'))\n",
    "    \n",
    "    \n",
    "    model.add(Conv1D(filters=256, kernel_size=6, activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=3, strides=2, padding='same'))\n",
    "    \n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    \n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    \n",
    "    csv_logger = CSVLogger('cnn_training_log.csv', append=True)\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        'best_cnn_model.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    \n",
    "    print(\"\\nTraining CNN model...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_cnn, y_train_onehot,\n",
    "        epochs=30,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_test_cnn, y_test_onehot),\n",
    "        callbacks=[csv_logger, checkpoint, early_stopping],\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Model trained in {training_time:.2f} seconds\")\n",
    "    \n",
    "    \n",
    "    print(\"\\nEvaluating CNN model on test data...\")\n",
    "    scores = model.evaluate(X_test_cnn, y_test_onehot, verbose=0)\n",
    "    print(f\"Test Accuracy: {scores[1]:.4f}\")\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    model.save('cnn_model.h5')\n",
    "    print(\"CNN model saved to 'cnn_model.h5'\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "\n",
    "def create_interactive_predictor(rf_model, cnn_model, data_dict):\n",
    "    \"\"\"\n",
    "    Create an interactive interface for predicting individual samples\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"INTERACTIVE PREDICTION INTERFACE\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    \n",
    "    scaler = joblib.load('feature_scaler.pkl')\n",
    "    label_encoder = data_dict['label_encoder']\n",
    "    feature_names = data_dict['feature_names']\n",
    "    \n",
    "    \n",
    "    \n",
    "    if isinstance(data_dict['X_test'], np.ndarray):\n",
    "        X_test_df = pd.DataFrame(data_dict['X_test'], columns=feature_names)\n",
    "    else:\n",
    "        X_test_df = data_dict['X_test']\n",
    "        \n",
    "    if isinstance(data_dict['y_test'], np.ndarray):\n",
    "        y_test_series = pd.Series(data_dict['y_test'])\n",
    "    else:\n",
    "        y_test_series = data_dict['y_test']\n",
    "    \n",
    "    \n",
    "    def predict_sample(sample_data):\n",
    "        \n",
    "        sample_array = np.array(sample_data).reshape(1, -1)\n",
    "        sample_scaled = scaler.transform(sample_array)\n",
    "        \n",
    "        \n",
    "        rf_pred = rf_model.predict(sample_scaled)[0]\n",
    "        rf_prob = rf_model.predict_proba(sample_scaled)[0]\n",
    "        \n",
    "        \n",
    "        sample_cnn = sample_scaled.reshape(1, sample_scaled.shape[1], 1)\n",
    "        cnn_prob = cnn_model.predict(sample_cnn)[0]\n",
    "        cnn_pred = np.argmax(cnn_prob)\n",
    "        \n",
    "        \n",
    "        rf_label = label_encoder.inverse_transform([rf_pred])[0]\n",
    "        cnn_label = label_encoder.inverse_transform([cnn_pred])[0]\n",
    "        \n",
    "        return {\n",
    "            'rf_prediction': rf_label,\n",
    "            'rf_confidence': np.max(rf_prob) * 100,\n",
    "            'cnn_prediction': cnn_label,\n",
    "            'cnn_confidence': np.max(cnn_prob) * 100,\n",
    "            'rf_probabilities': rf_prob,\n",
    "            'cnn_probabilities': cnn_prob\n",
    "        }\n",
    "    \n",
    "    \n",
    "    max_idx = min(len(X_test_df) - 1, len(y_test_series) - 1)  \n",
    "    sample_idx = np.random.randint(0, max_idx + 1)  \n",
    "    \n",
    "    \n",
    "    sample = X_test_df.iloc[sample_idx].values if hasattr(X_test_df, 'iloc') else X_test_df[sample_idx]\n",
    "    true_label_value = y_test_series.iloc[sample_idx] if hasattr(y_test_series, 'iloc') else y_test_series[sample_idx]\n",
    "    true_label = label_encoder.inverse_transform([true_label_value])[0]\n",
    "    \n",
    "    prediction_results = predict_sample(sample)\n",
    "    \n",
    "    print(\"\\nSample Prediction Demo:\")\n",
    "    print(f\"True label: {true_label}\")\n",
    "    print(f\"Random Forest prediction: {prediction_results['rf_prediction']} with {prediction_results['rf_confidence']:.2f}% confidence\")\n",
    "    print(f\"CNN prediction: {prediction_results['cnn_prediction']} with {prediction_results['cnn_confidence']:.2f}% confidence\")\n",
    "    \n",
    "    \n",
    "    labels = label_encoder.classes_\n",
    "    \n",
    "    \n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Random Forest Probabilities\", \"CNN Probabilities\"))\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=labels,\n",
    "            y=prediction_results['rf_probabilities'],\n",
    "            marker_color='blue',\n",
    "            name='Random Forest'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=labels,\n",
    "            y=prediction_results['cnn_probabilities'],\n",
    "            marker_color='red',\n",
    "            name='CNN'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_text=\"Prediction Probabilities Comparison\",\n",
    "        height=500,\n",
    "        width=1000,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    \n",
    "    print(\"\\nEnter custom sample features for prediction:\")\n",
    "    \n",
    "    \n",
    "    feature_widgets = {}\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        feature_val = float(sample[i]) if i < len(sample) else 0.0  \n",
    "        feature_widgets[feature] = widgets.FloatText(\n",
    "            value=feature_val,\n",
    "            description=f\"{feature}:\",\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='80%')\n",
    "        )\n",
    "    \n",
    "    \n",
    "    output_text = widgets.Output()\n",
    "    output_graph = widgets.Output()\n",
    "    \n",
    "    \n",
    "    def on_predict_button_clicked(b):\n",
    "        with output_text:\n",
    "            clear_output()\n",
    "            \n",
    "            custom_sample = [feature_widgets[feature].value for feature in feature_names]\n",
    "            results = predict_sample(custom_sample)\n",
    "            \n",
    "            print(f\"Random Forest prediction: {results['rf_prediction']} with {results['rf_confidence']:.2f}% confidence\")\n",
    "            print(f\"CNN prediction: {results['cnn_prediction']} with {results['cnn_confidence']:.2f}% confidence\")\n",
    "        \n",
    "        with output_graph:\n",
    "            clear_output()\n",
    "            \n",
    "            fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Random Forest Probabilities\", \"CNN Probabilities\"))\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=labels,\n",
    "                    y=results['rf_probabilities'],\n",
    "                    marker_color='blue',\n",
    "                    name='Random Forest'\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=labels,\n",
    "                    y=results['cnn_probabilities'],\n",
    "                    marker_color='red',\n",
    "                    name='CNN'\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "            \n",
    "            fig.update_layout(\n",
    "                title_text=\"Prediction Probabilities Comparison\",\n",
    "                height=500,\n",
    "                width=1000,\n",
    "                showlegend=True\n",
    "            )\n",
    "            \n",
    "            fig.show()\n",
    "    \n",
    "    \n",
    "    predict_button = widgets.Button(\n",
    "        description='Predict',\n",
    "        button_style='success',\n",
    "        tooltip='Click to predict'\n",
    "    )\n",
    "    predict_button.on_click(on_predict_button_clicked)\n",
    "    \n",
    "    \n",
    "    \n",
    "    feature_chunks = [feature_names[i:i+5] for i in range(0, len(feature_names), 5)]\n",
    "    \n",
    "    \n",
    "    tabs = widgets.Tab()\n",
    "    tab_children = []\n",
    "    \n",
    "    for i, chunk in enumerate(feature_chunks):\n",
    "        chunk_container = widgets.VBox([feature_widgets[f] for f in chunk])\n",
    "        tab_children.append(chunk_container)\n",
    "        \n",
    "    tabs.children = tab_children\n",
    "    \n",
    "    \n",
    "    for i in range(len(feature_chunks)):\n",
    "        tabs.set_title(i, f'Features {i*5+1}-{min((i+1)*5, len(feature_names))}')\n",
    "    \n",
    "    \n",
    "    layout = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Insider Threat Detection - Interactive Predictor</h3>\"),\n",
    "        widgets.HTML(\"<p>Adjust feature values and click Predict to see the model results</p>\"),\n",
    "        tabs,\n",
    "        predict_button,\n",
    "        widgets.HTML(\"<h4>Prediction Results:</h4>\"),\n",
    "        output_text,\n",
    "        output_graph\n",
    "    ])\n",
    "    \n",
    "    display(layout)\n",
    "    \n",
    "    \n",
    "    on_predict_button_clicked(None)\n",
    "    \n",
    "    return predict_sample\n",
    "\n",
    "\n",
    "def run_insider_threat_detection():\n",
    "    \"\"\"\n",
    "    Main function to run the entire pipeline\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"AI-POWERED INSIDER THREAT DETECTION SYSTEM\")\n",
    "    print(\"Based on CICIDS 2018 Dataset\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    \n",
    "    file_path = '/home/heheboi/Desktop/IS Project/IDS Intrusion Dataset/02-14-2018.csv'  \n",
    "    df = load_data(file_path)\n",
    "    \n",
    "    if df is None:\n",
    "        print(\"Error loading data. Please check the file path.\")\n",
    "        return\n",
    "        \n",
    "    \n",
    "    perform_eda(df)\n",
    "    \n",
    "    \n",
    "    data_dict = preprocess_data(df)\n",
    "    \n",
    "    \n",
    "    rf_model = build_rf_model(data_dict)\n",
    "    \n",
    "    \n",
    "    cnn_model, history = build_cnn_model(data_dict)\n",
    "    \n",
    "    \n",
    "    create_interactive_predictor(rf_model, cnn_model, data_dict)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PROJECT COMPLETE\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return {\n",
    "        'rf_model': rf_model,\n",
    "        'cnn_model': cnn_model,\n",
    "        'data_dict': data_dict\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_insider_threat_detection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
